{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/python-freak/CBOW-Implementation-/blob/master/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oqJHFdxgHmVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Implementation of Continuous bag of Words Model"
      ]
    },
    {
      "metadata": {
        "id": "V5AGO-Z-HmVe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Importing dependencies"
      ]
    },
    {
      "metadata": {
        "id": "3O_ntjAzHmVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding , Dense , Flatten , Lambda\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import keras.backend as k\n",
        "from keras.models import Sequential\n",
        "import re\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WfS2rV_7HmVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0c253388-f401-413b-81b1-183157f494bf"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "0K2kalwOHmVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HlfpHHYTHmVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "24b41d34-b1aa-4863-c25c-8fd8662b542f"
      },
      "cell_type": "code",
      "source": [
        "gutenberg.fileids()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "bTc1L7U1HmVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_text = gutenberg.raw('bible-kjv.txt')\n",
        "raw_text = re.sub('\\n',' ',raw_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg8WNxwEHmVy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re \n",
        "corpus = re.compile('[0-9]+:[0-9]+').split(raw_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPqFSkO5HmV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(corpus)\n",
        "corpus = corpus[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JfazlJDEHmV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = text.Tokenizer(num_words = 5000)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "word2id = tokenizer.word_index\n",
        "word2id['<PAD>'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBfvpQhCHmWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1918a236-116e-47e5-99c3-3141136f3458"
      },
      "cell_type": "code",
      "source": [
        "len(word2id)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6823"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "RkBlujyFHmWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61f00727-2c0b-41f4-fa91-84f25f245290"
      },
      "cell_type": "code",
      "source": [
        "id2word = {id:word for word , id in word2id.items()}\n",
        "type(word2id)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "Jzjaoz7kHmWL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2id)\n",
        "embedding_size = 128\n",
        "window_size = 2\n",
        "num_epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7tGLdb8SHmWP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wids = [[word2id[word] for word in text.text_to_word_sequence(doc)] for doc in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eB-q8Z_KHmWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d79b4e2-d66f-4811-eabb-ca5e21d4168f"
      },
      "cell_type": "code",
      "source": [
        "len(wids[4])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "RDCibqjRHmWY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  defining function to generate context and target word pairs"
      ]
    },
    {
      "metadata": {
        "id": "ZybQWs0-HmWZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_context_target_word_pairs(corpus , vocab_size , window_size):\n",
        "    context_length = 2*window_size\n",
        "    for words in corpus:\n",
        "        sequence_length = len(words)\n",
        "        for index , word in enumerate(words):\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1 \n",
        "            context_words = []\n",
        "            label_word = []\n",
        "            context_words.append([words[i] for i in range(start,end) if 0 <= i < sequence_length and i != index])\n",
        "            label_word.append(word)\n",
        "            #x = context_words\n",
        "            x = pad_sequences(context_words , maxlen = context_length , padding = 'pre' , truncating = 'pre')\n",
        "            y = to_categorical(label_word , vocab_size)\n",
        "            yield(x,y)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NV-pKUPYHmWd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Generating Context and target pairs"
      ]
    },
    {
      "metadata": {
        "id": "yVOXYMUQHmWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "7c0f63e0-edda-40eb-aaad-6fae4339aebd"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "i = 0\n",
        "for x,y in generate_context_target_word_pairs(corpus = wids , vocab_size = vocab_size , window_size = window_size):\n",
        "    if 0 not in x[0]:\n",
        "        \n",
        "        print(\"Context Words :\" , [id2word[i] for i in x[0]] , '-> Target Word :' , id2word[np.argmax(y)])\n",
        "        if i == 20:\n",
        "            break\n",
        "        i = i + 1\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context Words : ['the', 'king', 'bible', 'the'] -> Target Word : james\n",
            "Context Words : ['king', 'james', 'the', 'old'] -> Target Word : bible\n",
            "Context Words : ['james', 'bible', 'old', 'testament'] -> Target Word : the\n",
            "Context Words : ['bible', 'the', 'testament', 'of'] -> Target Word : old\n",
            "Context Words : ['the', 'old', 'of', 'the'] -> Target Word : testament\n",
            "Context Words : ['old', 'testament', 'the', 'king'] -> Target Word : of\n",
            "Context Words : ['testament', 'of', 'king', 'james'] -> Target Word : the\n",
            "Context Words : ['of', 'the', 'james', 'bible'] -> Target Word : king\n",
            "Context Words : ['the', 'king', 'bible', 'the'] -> Target Word : james\n",
            "Context Words : ['king', 'james', 'the', 'first'] -> Target Word : bible\n",
            "Context Words : ['james', 'bible', 'first', 'book'] -> Target Word : the\n",
            "Context Words : ['bible', 'the', 'book', 'of'] -> Target Word : first\n",
            "Context Words : ['the', 'first', 'of', 'moses'] -> Target Word : book\n",
            "Context Words : ['first', 'book', 'moses', 'called'] -> Target Word : of\n",
            "Context Words : ['book', 'of', 'called', 'genesis'] -> Target Word : moses\n",
            "Context Words : ['in', 'the', 'god', 'created'] -> Target Word : beginning\n",
            "Context Words : ['the', 'beginning', 'created', 'the'] -> Target Word : god\n",
            "Context Words : ['beginning', 'god', 'the', 'heaven'] -> Target Word : created\n",
            "Context Words : ['god', 'created', 'heaven', 'and'] -> Target Word : the\n",
            "Context Words : ['created', 'the', 'and', 'the'] -> Target Word : heaven\n",
            "Context Words : ['the', 'heaven', 'the', 'earth'] -> Target Word : and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gRnRmUs0HmWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "e3c53e2e-23bb-49e7-dfa4-97cc6039e09b"
      },
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size , output_dim = embedding_size , input_length = window_size * 2))\n",
        "model.add(Lambda(lambda x: K.mean(x , axis  = 1) , output_shape = (embedding_size ,)))\n",
        "model.add(Dense(output_dim = vocab_size , activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy' , optimizer = 'rmsprop')\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 4, 128)            873344    \n",
            "_________________________________________________________________\n",
            "lambda_4 (Lambda)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6823)              880167    \n",
            "=================================================================\n",
            "Total params: 1,753,511\n",
            "Trainable params: 1,753,511\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=6823)`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xnr4EeGJHmWr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "uYsWt3dmHmWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "3c20047a-0abd-469d-a1e7-7e20fdf9a876"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    loss = 0\n",
        "    i = 0 \n",
        "    for x , y in generate_context_target_word_pairs(corpus = wids , vocab_size = vocab_size , window_size = window_size):\n",
        "        i = i + 1\n",
        "        loss+=model.train_on_batch(x,y)\n",
        "    print('Epoch :' , epoch , '-> Loss :' , loss)\n",
        "    print()\n",
        "            "
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0 -> Loss : 2187087.5277019152\n",
            "\n",
            "Epoch : 1 -> Loss : 2326781.2880545533\n",
            "\n",
            "Epoch : 2 -> Loss : 2341920.4156878483\n",
            "\n",
            "Epoch : 3 -> Loss : 2296029.482446004\n",
            "\n",
            "Epoch : 4 -> Loss : 2271344.861756953\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NFncQJeSHmW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "88b631c0-d8c2-443c-9d65-65c1e51e0f4b"
      },
      "cell_type": "code",
      "source": [
        "weights = model.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6822, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>1.324713</td>\n",
              "      <td>-2.262283</td>\n",
              "      <td>1.599976</td>\n",
              "      <td>2.406861</td>\n",
              "      <td>-2.780106</td>\n",
              "      <td>-2.143501</td>\n",
              "      <td>2.811772</td>\n",
              "      <td>-1.582393</td>\n",
              "      <td>3.211106</td>\n",
              "      <td>-1.832466</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.941061</td>\n",
              "      <td>2.641529</td>\n",
              "      <td>2.236893</td>\n",
              "      <td>-2.192495</td>\n",
              "      <td>1.879202</td>\n",
              "      <td>-1.396138</td>\n",
              "      <td>-1.442995</td>\n",
              "      <td>-2.767183</td>\n",
              "      <td>1.814842</td>\n",
              "      <td>1.900154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>4.704708</td>\n",
              "      <td>-2.103935</td>\n",
              "      <td>4.209386</td>\n",
              "      <td>3.172010</td>\n",
              "      <td>-1.985346</td>\n",
              "      <td>-2.742967</td>\n",
              "      <td>2.615588</td>\n",
              "      <td>-3.135289</td>\n",
              "      <td>2.659963</td>\n",
              "      <td>-2.564027</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.873548</td>\n",
              "      <td>3.318985</td>\n",
              "      <td>2.715850</td>\n",
              "      <td>-2.700720</td>\n",
              "      <td>2.632065</td>\n",
              "      <td>-2.791503</td>\n",
              "      <td>-3.126455</td>\n",
              "      <td>-2.987996</td>\n",
              "      <td>3.810323</td>\n",
              "      <td>2.924502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>2.862848</td>\n",
              "      <td>-1.886645</td>\n",
              "      <td>3.289742</td>\n",
              "      <td>2.582556</td>\n",
              "      <td>-1.442604</td>\n",
              "      <td>-2.302830</td>\n",
              "      <td>2.404355</td>\n",
              "      <td>-0.168917</td>\n",
              "      <td>2.604083</td>\n",
              "      <td>-3.022157</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.886205</td>\n",
              "      <td>0.728539</td>\n",
              "      <td>2.210032</td>\n",
              "      <td>-2.229585</td>\n",
              "      <td>1.911665</td>\n",
              "      <td>-2.586068</td>\n",
              "      <td>-2.152974</td>\n",
              "      <td>-2.254407</td>\n",
              "      <td>2.624727</td>\n",
              "      <td>1.430733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>2.384757</td>\n",
              "      <td>-1.404098</td>\n",
              "      <td>1.982178</td>\n",
              "      <td>2.229606</td>\n",
              "      <td>-0.802226</td>\n",
              "      <td>-1.819652</td>\n",
              "      <td>1.763981</td>\n",
              "      <td>-1.855353</td>\n",
              "      <td>2.099869</td>\n",
              "      <td>-2.373384</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.535964</td>\n",
              "      <td>0.689754</td>\n",
              "      <td>1.109069</td>\n",
              "      <td>-1.664946</td>\n",
              "      <td>1.315290</td>\n",
              "      <td>-2.239972</td>\n",
              "      <td>-1.253184</td>\n",
              "      <td>-1.127031</td>\n",
              "      <td>1.344183</td>\n",
              "      <td>1.796830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>that</th>\n",
              "      <td>2.181835</td>\n",
              "      <td>-1.509150</td>\n",
              "      <td>2.057486</td>\n",
              "      <td>1.349586</td>\n",
              "      <td>-0.160132</td>\n",
              "      <td>-1.812121</td>\n",
              "      <td>1.953357</td>\n",
              "      <td>-1.343929</td>\n",
              "      <td>2.245456</td>\n",
              "      <td>-1.927707</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.380958</td>\n",
              "      <td>0.703841</td>\n",
              "      <td>0.426059</td>\n",
              "      <td>-1.262542</td>\n",
              "      <td>0.852948</td>\n",
              "      <td>-2.644204</td>\n",
              "      <td>-0.631043</td>\n",
              "      <td>-1.140461</td>\n",
              "      <td>1.523414</td>\n",
              "      <td>0.772313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "and   1.324713 -2.262283  1.599976  2.406861 -2.780106 -2.143501  2.811772   \n",
              "of    4.704708 -2.103935  4.209386  3.172010 -1.985346 -2.742967  2.615588   \n",
              "to    2.862848 -1.886645  3.289742  2.582556 -1.442604 -2.302830  2.404355   \n",
              "in    2.384757 -1.404098  1.982178  2.229606 -0.802226 -1.819652  1.763981   \n",
              "that  2.181835 -1.509150  2.057486  1.349586 -0.160132 -1.812121  1.953357   \n",
              "\n",
              "           7         8         9      ...          118       119       120  \\\n",
              "and  -1.582393  3.211106 -1.832466    ...    -1.941061  2.641529  2.236893   \n",
              "of   -3.135289  2.659963 -2.564027    ...    -2.873548  3.318985  2.715850   \n",
              "to   -0.168917  2.604083 -3.022157    ...    -1.886205  0.728539  2.210032   \n",
              "in   -1.855353  2.099869 -2.373384    ...    -0.535964  0.689754  1.109069   \n",
              "that -1.343929  2.245456 -1.927707    ...    -0.380958  0.703841  0.426059   \n",
              "\n",
              "           121       122       123       124       125       126       127  \n",
              "and  -2.192495  1.879202 -1.396138 -1.442995 -2.767183  1.814842  1.900154  \n",
              "of   -2.700720  2.632065 -2.791503 -3.126455 -2.987996  3.810323  2.924502  \n",
              "to   -2.229585  1.911665 -2.586068 -2.152974 -2.254407  2.624727  1.430733  \n",
              "in   -1.664946  1.315290 -2.239972 -1.253184 -1.127031  1.344183  1.796830  \n",
              "that -1.262542  0.852948 -2.644204 -0.631043 -1.140461  1.523414  0.772313  \n",
              "\n",
              "[5 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "metadata": {
        "id": "zhqWgQ-rPfED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "d2e5dbfb-f06a-418b-954a-faa343e907a8"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# compute pairwise distance matrix\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "# view contextually similar words\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['god', 'noah', 'egypt', 'commanded']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6822, 6822)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'commanded': ['sent', 'more', 'delivered', 'given', 'bless'],\n",
              " 'egypt': ['canaan', 'whither', 'gilead', 'way', 'goshen'],\n",
              " 'god': ['because', 'against', 'out', 'hath', 'or'],\n",
              " 'noah': ['azariah', 'mahlon', 'shem', 'obadiah', 'hadad']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    }
  ]
}